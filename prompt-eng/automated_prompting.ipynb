{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Prompting\n",
    "\n",
    "Automated prompting is an advanced technique in prompt engineering, introduced by [Zhang et al. (2022)](https://arxiv.org/abs/2211.01910), that utilizes a GenAI model to automatically generate and evaluate well structured prompts to be used within the same or other models.\n",
    "\n",
    "For the purposes of this experiment, we are only creating a single automated prompt in a zero shot style using meta prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': 'You are an expert in software requirement analysis. Your task is to generate a well-structured zero-shot prompt that guides an AI model to analyze and extract key requirements for a following solution:\\nA study companion discord bot that is to accept any document, analyse it, and answer questions based on the document.\\nThe generated prompt must instruct the AI to Identify User Interactions (List all distinct interactions that users will perform within the system) and Define Supporting Functionalities (Specify the essential system functionalities required to enable these interactions). The generated prompt must exclude non-essential details such as system architecture, implementation details, or non-functional requirements, and be formatted as a standalone prompt with no additional explanation, introductory text, or metadata. Output only the generated prompt and nothing else.', 'stream': False, 'options': {'temperature': 0.3, 'num_ctx': 512, 'num_predict': 250}}\n",
      "Automatically Generated Prompt:\n",
      "Identify distinct user interactions within the study companion discord bot, including but not limited to:\n",
      "- Sending document files for analysis\n",
      "- Asking specific questions about the document content\n",
      "- Receiving answers and explanations from the bot\n",
      "- Requesting clarification or additional information\n",
      "- Reporting errors or inconsistencies in the analysis\n",
      "\n",
      "Define essential system functionalities required to enable these user interactions, focusing on key features such as:\n",
      "- Document file parsing and analysis capabilities\n",
      "- Natural language processing for question answering and explanation generation\n",
      "- Error detection and reporting mechanisms\n",
      "- User interface for inputting questions and receiving results\n",
      "Time taken: 4.32s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'model': 'llama3.2:latest', 'prompt': 'Identify distinct user interactions within the study companion discord bot, including but not limited to:\\n- Sending document files for analysis\\n- Asking specific questions about the document content\\n- Receiving answers and explanations from the bot\\n- Requesting clarification or additional information\\n- Reporting errors or inconsistencies in the analysis\\n\\nDefine essential system functionalities required to enable these user interactions, focusing on key features such as:\\n- Document file parsing and analysis capabilities\\n- Natural language processing for question answering and explanation generation\\n- Error detection and reporting mechanisms\\n- User interface for inputting questions and receiving results', 'stream': False, 'options': {'temperature': 0.2, 'num_ctx': 1024, 'num_predict': 500}}\n",
      "Model Response:\n",
      "Here are the distinct user interactions within the study companion Discord bot:\n",
      "\n",
      "1. **Sending document files for analysis**: Users can upload or share documents (e.g., PDFs, Word documents) with the bot to analyze their content.\n",
      "2. **Asking specific questions about the document content**: Users can ask the bot specific questions about the document, such as \"What is the main topic of this article?\" or \"Can you summarize the key points?\"\n",
      "3. **Receiving answers and explanations from the bot**: The bot responds with relevant answers and explanations to the user's questions.\n",
      "4. **Requesting clarification or additional information**: Users can ask for clarification on specific points or request additional information related to their question.\n",
      "5. **Reporting errors or inconsistencies in the analysis**: Users can report any errors or inconsistencies found during the analysis, allowing the bot to correct its understanding and provide more accurate results.\n",
      "\n",
      "To enable these user interactions, the following essential system functionalities are required:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Document File Parsing and Analysis Capabilities**:\n",
      "\t* Ability to parse and extract relevant information from uploaded documents (e.g., text extraction, entity recognition)\n",
      "\t* Natural Language Processing (NLP) capabilities for document analysis (e.g., sentiment analysis, topic modeling)\n",
      "2. **Natural Language Processing for Question Answering and Explanation Generation**:\n",
      "\t* NLP algorithms for question answering (e.g., intent detection, entity disambiguation)\n",
      "\t* Text generation capabilities for providing explanations and answers to user questions\n",
      "3. **Error Detection and Reporting Mechanisms**:\n",
      "\t* Automated error detection mechanisms for identifying inconsistencies or errors in the analysis\n",
      "\t* User reporting mechanism for users to report errors or inconsistencies found during analysis\n",
      "4. **User Interface for Inputting Questions and Receiving Results**:\n",
      "\t* Command-line interface (CLI) or graphical user interface (GUI) for users to input questions and receive results\n",
      "\t* Integration with Discord's API for seamless interaction within the platform\n",
      "\n",
      "**Additional Requirements:**\n",
      "\n",
      "1. **Data Storage and Retrieval**: Ability to store and retrieve analyzed documents and user interactions for future reference and improvement.\n",
      "2. **Security and Authentication**: Implement security measures (e.g., encryption, access controls) to protect user data and ensure only authorized users can interact with the bot.\n",
      "3. **Scalability and Performance**: Design the system to handle a large volume of user interactions and document uploads while maintaining performance and response times.\n",
      "\n",
      "By incorporating these essential system functionalities\n",
      "Time taken: 7.187s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## AUTOMATED PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Create an inbound prompt\n",
    "MESSAGE = \"A study companion discord bot that is to accept any document, analyse it, and answer questions based on the document.\"\n",
    "\n",
    "#### (2) Simulate a Workflow Template\n",
    "TEMPLATE_BEFORE=\"You are an expert in software requirement analysis. Your task is to generate a well-structured zero-shot prompt that guides an AI model to analyze and extract key requirements for a following solution:\"\n",
    "TEMPLATE_AFTER= (\"The generated prompt must instruct the AI to Identify User Interactions (List all distinct interactions that users will perform within the system) \" \n",
    "                 \"and Define Supporting Functionalities (Specify the essential system functionalities required to enable these interactions). \"\n",
    "                 \"The generated prompt must exclude non-essential details such as system architecture, implementation details, or non-functional requirements, \"\n",
    "                 \"and be formatted as a standalone prompt with no additional explanation, introductory text, or metadata. Output only the generated prompt and nothing else.\")\n",
    "PROMPT = TEMPLATE_BEFORE + '\\n' + MESSAGE + '\\n' + TEMPLATE_AFTER\n",
    "\n",
    "#### (3) Create a first payload\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, # Responsible for how \"random\" can the answer be. Keeping at 0.3 to keep prompt consistant yet allow for small variations.\n",
    "                         num_ctx=512, # Essentially responsible for models memory / context window. 512 allows the model to create a consize response with some extra room if needed.\n",
    "                         num_predict=250) # Responsible for how many word tokens are permitted to be returned. 250 keeps the prompt consize.\n",
    "\n",
    "# Send out to the first model\n",
    "time, response = model_req(payload=payload)\n",
    "print(\"Automatically Generated Prompt:\\n\" + response)\n",
    "if time: print(f'Time taken: {time}s')\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "#### (4) Create a second payload\n",
    "automatedPayload = create_payload(target=\"ollama\",\n",
    "                                model=\"llama3.2:latest\", \n",
    "                                prompt=response, \n",
    "                                temperature=0.2, # Responsible for how \"random\" can the answer be. Keeping at 0.2 to keep answers consistant\n",
    "                                num_ctx=1024, # Essentially responsible for models memory / context window. 1024 ensures the model fully understands both the meta-prompt and the requirement analysis.\n",
    "                                num_predict=500) # Responsible for how many word tokens are permitted to be returned. 500 keeps the answer consize and to the point.\n",
    "\n",
    "# Send out to the second model\n",
    "automatedTime, automatedResponse = model_req(payload=automatedPayload)\n",
    "print(\"Model Response:\\n\" + automatedResponse)\n",
    "if automatedTime: print(f'Time taken: {automatedTime}s')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
