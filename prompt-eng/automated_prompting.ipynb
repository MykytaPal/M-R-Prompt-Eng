{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Prompting\n",
    "\n",
    "Automated prompting is an advanced technique in prompt engineering, introduced by [Zhang et al. (2022)](https://arxiv.org/abs/2211.01910), that utilizes a GenAI model to automatically generate and evaluate well structured prompts to be used within the same or other models.\n",
    "\n",
    "For the purposes of this experiment, we are only creating a single automated prompt in a zero shot style using meta prompting, formatting it to remove unnecessary reponse elements, and feed it back into the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': 'You are an expert in software requirement analysis. Your task is to generate a well-structured zero-shot prompt that guides an AI model to analyze and extract key requirements for a following solution:\\nA study companion discord bot that is to accept any document, analyse it, and answer questions based on the document.\\nThe generated prompt must instruct the AI to Identify User Interactions (List all distinct interactions that users will perform within the system) and Define Supporting Functionalities (Specify the essential system functionalities required to enable these interactions). The generated prompt must exclude non-essential details such as system architecture, implementation details, or non-functional requirements, and be formatted as a standalone prompt with no additional explanation, introductory text, or metadata. Output only the generated prompt and nothing else.', 'stream': False, 'options': {'temperature': 0.3, 'num_ctx': 512, 'num_predict': 250}}\n",
      "Automatically Generated Prompt:\n",
      "Identify distinct user interactions within the study companion discord bot, including but not limited to:\n",
      "- Sending document files for analysis\n",
      "- Asking specific questions about the document content\n",
      "- Receiving answers and explanations from the bot\n",
      "- Requesting clarification or additional information\n",
      "- Reporting errors or inconsistencies in the analysis\n",
      "\n",
      "Define essential system functionalities required to enable these user interactions, focusing on key features such as:\n",
      "- Document file parsing and analysis capabilities\n",
      "- Natural language processing for question answering and explanation generation\n",
      "- Error detection and reporting mechanisms\n",
      "- User interface for inputting questions and receiving results\n",
      "Time taken: 4.32s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'model': 'llama3.2:latest', 'prompt': 'Identify distinct user interactions within the study companion discord bot, including but not limited to:\\n- Sending document files for analysis\\n- Asking specific questions about the document content\\n- Receiving answers and explanations from the bot\\n- Requesting clarification or additional information\\n- Reporting errors or inconsistencies in the analysis\\n\\nDefine essential system functionalities required to enable these user interactions, focusing on key features such as:\\n- Document file parsing and analysis capabilities\\n- Natural language processing for question answering and explanation generation\\n- Error detection and reporting mechanisms\\n- User interface for inputting questions and receiving results', 'stream': False, 'options': {'temperature': 0.2, 'num_ctx': 1024, 'num_predict': 500}}\n",
      "Model Response:\n",
      "Here are the distinct user interactions within the study companion Discord bot:\n",
      "\n",
      "1. **Sending document files for analysis**: Users can upload or share documents (e.g., PDFs, Word documents) with the bot to analyze their content.\n",
      "2. **Asking specific questions about the document content**: Users can ask the bot specific questions about the document, such as \"What is the main topic of this article?\" or \"Can you summarize the key points?\"\n",
      "3. **Receiving answers and explanations from the bot**: The bot responds with relevant answers and explanations to the user's questions.\n",
      "4. **Requesting clarification or additional information**: Users can ask for clarification on specific points or request additional information related to their question.\n",
      "5. **Reporting errors or inconsistencies in the analysis**: Users can report any errors or inconsistencies found during the analysis, allowing the bot to correct its understanding and provide more accurate results.\n",
      "\n",
      "To enable these user interactions, the following essential system functionalities are required:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Document File Parsing and Analysis Capabilities**:\n",
      "\t* Ability to parse and extract relevant information from uploaded documents (e.g., text extraction, entity recognition)\n",
      "\t* Natural Language Processing (NLP) capabilities for document analysis (e.g., sentiment analysis, topic modeling)\n",
      "2. **Natural Language Processing for Question Answering and Explanation Generation**:\n",
      "\t* NLP algorithms for question answering (e.g., intent detection, entity disambiguation)\n",
      "\t* Text generation capabilities for providing explanations and answers to user questions\n",
      "3. **Error Detection and Reporting Mechanisms**:\n",
      "\t* Automated error detection mechanisms for identifying inconsistencies or errors in the analysis\n",
      "\t* User reporting mechanism for users to report errors or inconsistencies found during analysis\n",
      "4. **User Interface for Inputting Questions and Receiving Results**:\n",
      "\t* Command-line interface (CLI) or graphical user interface (GUI) for users to input questions and receive results\n",
      "\t* Integration with Discord's API for seamless interaction within the platform\n",
      "\n",
      "**Additional Requirements:**\n",
      "\n",
      "1. **Data Storage and Retrieval**: Ability to store and retrieve analyzed documents and user interactions for future reference and improvement.\n",
      "2. **Security and Authentication**: Implement security measures (e.g., encryption, access controls) to protect user data and ensure only authorized users can interact with the bot.\n",
      "3. **Scalability and Performance**: Design the system to handle a large volume of user interactions and document uploads while maintaining performance and response times.\n",
      "\n",
      "By incorporating these essential system functionalities\n",
      "Time taken: 7.187s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## AUTOMATED PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Create an inbound prompt\n",
    "#### In this case, it was designed to be short and consise, without any extra prompting specifications as it is being handled by a template.\n",
    "MESSAGE = \"A study companion discord bot that is to accept any document, analyse it, and answer questions based on the document.\"\n",
    "\n",
    "#### (2) Simulate a Workflow Template. \n",
    "#### In this case, a Template is being used to simplify a process of Requirement Analysis for a user and make it universal for any project / solution.\n",
    "TEMPLATE_BEFORE=\"You are an expert in software requirement analysis. Your task is to generate a well-structured zero-shot prompt that guides an AI model to analyze and extract key requirements for a following solution:\"\n",
    "TEMPLATE_AFTER= (\"The generated prompt must instruct the AI to Identify User Interactions (List all distinct interactions that users will perform within the system) \" \n",
    "                 \"and Define Supporting Functionalities (Specify the essential system functionalities required to enable these interactions). \"\n",
    "                 \"The generated prompt must exclude non-essential details such as system architecture, implementation details, or non-functional requirements, \"\n",
    "                 \"and be formatted as a standalone prompt with no additional explanation, introductory text, or metadata. Output only the generated prompt and nothing else.\")\n",
    "PROMPT = TEMPLATE_BEFORE + '\\n' + MESSAGE + '\\n' + TEMPLATE_AFTER\n",
    "\n",
    "#### The Template above has gone through multiple iterations of fine tuning along with help of automated prompting technique from ChatGPT. \n",
    "#### We set the GenAI's identity to help it narrow down its response pool, specify a desired prompting technique and request a prompt realted to Requirement Analysis.\n",
    "#### We then specify exact needs of our Requirement Analysis (User Interactions and Supporting Functionalities), and format it's output by cutting out any non essential analysis or text.\n",
    "\n",
    "#### (3) Create a first payload\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, # Responsible for how \"random\" can the answer be. Keeping at 0.3 to keep prompt consistant yet allow for small variations.\n",
    "                         num_ctx=512, # Essentially responsible for models memory / context window. 512 allows the model to create a consize response with some extra room if needed.\n",
    "                         num_predict=250) # Responsible for how many word tokens are permitted to be returned. 250 keeps the prompt consize.\n",
    "\n",
    "#### Upon runing multiple experiments with various parameters, we have found that:\n",
    "#### Temperature of 0-0.2 keeps answers consistant with no iterations, 0.3-0.5 adds some variations while keeping content the same, 0.5-1.0 adds high degree of variation which often causes model to go off topic.\n",
    "#### Context window is highly dependant on the task. While 100-200 might be enough for simple prompts and responses (such as the ones given in the example), more complex questions require larger context windows. That said, having it too high causes model to off topic.\n",
    "#### Number of tokens simply controls the allowed size of the output. Smaller numbers will give short and consize responses, while higher numbers will go into a lot more details. Context Window needs to be adjusted accordingly.\n",
    "\n",
    "# Send out to the first model\n",
    "time, response = model_req(payload=payload)\n",
    "print(\"Automatically Generated Prompt:\\n\" + response)\n",
    "if time: print(f'Time taken: {time}s')\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "#### (4) Create a second payload\n",
    "automatedPayload = create_payload(target=\"ollama\",\n",
    "                                model=\"llama3.2:latest\", \n",
    "                                prompt=response, \n",
    "                                temperature=0.2, # Responsible for how \"random\" can the answer be. Keeping at 0.2 to keep answers consistant\n",
    "                                num_ctx=1024, # Essentially responsible for models memory / context window. 1024 ensures the model fully understands both the meta-prompt and is capable of generating the requirement analysis.\n",
    "                                num_predict=500) # Responsible for how many word tokens are permitted to be returned. 500 keeps the requirement analysis consize and to the point.\n",
    "\n",
    "# Send out to the second model\n",
    "automatedTime, automatedResponse = model_req(payload=automatedPayload)\n",
    "print(\"Model Response:\\n\" + automatedResponse)\n",
    "if automatedTime: print(f'Time taken: {automatedTime}s')\n",
    "\n",
    "#### Please see below for a detailed analysis of the implemented technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Conclusions:\n",
    "\n",
    "In order to implement the automated prompting technique, we have used a combination of prompt templates, meta prompting, and zero-shot prompting to achieve desirable results.\n",
    "\n",
    "We are using prompt templates with meta prompting to generate a detailed zero-shot style prompt with minimal user input, which is then fed back into the model.\n",
    "\n",
    "Before assembling this prompting method, we first ran extensive tests on meta prompting and zero-shot prompting to identify the best model parameters for each part of the automated prompting pipeline. Below are our observations regarding model parameters.\n",
    "\n",
    "## Parameter Settings:\n",
    "\n",
    "Temperature:\n",
    "0.0 - 0.2 yield the most consistent results with little to no variation. We have found this range to be the most useful when generating the requirement analysis, as there is no need for any sort of \"randomness\" or \"creativity\" for this task.\n",
    "\n",
    "0.3 - 0.5 still yield consistent results, however, they allow for some variation, which we found useful when generating prompts themselves. Normally, automated prompting includes continuous prompt creation and evaluation, which allows the algorithm to pick the best prompt/answer pair, which is where a small degree of variation is beneficial.\n",
    "\n",
    "0.5 - 1.0 yields results with a high degree of variation, which often causes prompts and requirement analysis to go vastly off topic or cover irrelevant information. There were even cases where the AI refused to generate a prompt and instead generated portions of the requirement analysis. We find this range of temperatures to be the best for creative tasks.\n",
    "\n",
    "Context Window:\n",
    "Through our experiments, we have found the context window to be heavily dependent on the size and complexity of the task. Lower context windows benefit small and easy tasks (such as answering basic algebra questions), while larger context windows are required to properly analyze more complex tasks and generate appropriate responses.\n",
    "\n",
    "We have found that a range between 300-500 yielded the best results for prompt generation, as it allowed the AI to properly comprehend the given prompt and provide an appropriate result. Setting it to lower sizes made the AI struggle with either prompt comprehension or prompt generation, while higher sizes used unnecessary resources and sometimes made the AI \"overthink\" and go off topic.\n",
    "\n",
    "A range between 750-1000 has yielded the best results for requirement analysis for the same reasons as specified above.\n",
    "\n",
    "Token Generation Limit:\n",
    "Through our experiments and research, we have found this to be heavily dependent on the task at hand, more specifically on how detailed a user wants an output to be. Smaller limits create concise responses, while higher limits allow for a lot more detail, examples, and explanations.\n",
    "\n",
    "For prompt generation, we have found 200-300 tokens to be plentiful, if not a little redundant. Going below 200 sometimes limited the details that needed to be included in the prompt, and going over 300 sometimes caused unnecessary details to appear. 250 tokens seemed to consistently bring the best results.\n",
    "\n",
    "For requirement analysis, 400-600 tokens seemed to give the best results, allowing for a detailed list of interactions and functionalities along with some descriptions; however, sometimes it would get off topic and start including unnecessary information such as \"Extra Considerations\" or \"Examples\", etc. 500 tokens seemed to be a good middle ground, although even that has given issues at times.\n",
    "\n",
    "## Performance and Behavior:\n",
    "\n",
    "Overall, this prompting technique implementation has yielded decent performance. It successfully generates a well-defined prompt, and the created prompt produces a decent surface-level requirement analysis. That being said, the requirement analysis could definitely go into more detail, and the model tends to include unnecessary information in the requirement analysis that was not requested. We attribute this to the lack of a prompt evaluation algorithm and to llama3.2 being a smaller and weaker model than what is required for this task.\n",
    "\n",
    "We would say that using this implementation of the automated prompting technique for requirement analysis could give users a good starting point, but it will not generate a complete analysis by itself.\n",
    "\n",
    "## Limitations:\n",
    "\n",
    "This implementation of automated prompting is not without its limitations:\n",
    "\n",
    "1. It uses the llama3.2 model, which has a limited data pool and reasoning capabilities.\n",
    "2. There is no prompt evaluation, which means that the end result will never be optimal.\n",
    "3. Automated prompting in general is not entirely optimal due to users' inability to alter the generated prompt to best fit the requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
