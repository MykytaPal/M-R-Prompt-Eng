{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Prompting\n",
    "\n",
    "Meta prompting is an advanced technique in prompt engineering that emphasizes the structural and syntactical organization of tasks and problems rather than focusing on their specific content. The objective is to create a more abstract, form-driven way of engaging with large language models (LLMs), highlighting patterns and structure over traditional content-focused methods.\n",
    "\n",
    "As outlined by [Zhang et al. (2024)](https://arxiv.org/abs/2311.11482), the defining features of meta prompting include:\n",
    "\n",
    "* Structure-Oriented: Prioritizes the organization and pattern of problems and solutions instead of specific content.\n",
    "* Syntax-Guided: Leverages syntax as a template to shape the expected responses or solutions.\n",
    "* Abstract Frameworks: Uses abstract examples as blueprints, demonstrating the structure of tasks without relying on concrete details.\n",
    "* Domain Versatility: Can be applied across multiple fields, offering structured solutions to diverse problem types.\n",
    "* Categorical Approach: Draws on type theory to organize and categorize components logically, enhancing prompt coherence and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': 'You are an expert in software requirement analysis. Your task is to generate a well-structured zero-shot prompt that guides an AI model to analyze and extract key requirements for a following solution: A study companion discord bot that is to accept any document, analyse it, and answer questions based on the document. The generated prompt must instruct the AI to Identify User Interactions (List all distinct interactions that users will perform within the system) and Define Supporting Functionalities (Specify the essential system functionalities required to enable these interactions). The generated prompt must exclude non-essential details such as system architecture, implementation details, or non-functional requirements, and be formatted as a standalone prompt with no additional explanation, introductory text, or metadata. Output only the generated prompt and nothing else.', 'stream': False, 'options': {'temperature': 0.3, 'num_ctx': 512, 'num_predict': 250}}\n",
      "Analyze the user interactions for a study companion Discord bot that accepts any document, analyzes it, and answers questions based on the document. Identify distinct interactions that users will perform within the system, including but not limited to: \n",
      "- Document upload\n",
      "- Query submission\n",
      "- Answer retrieval\n",
      "- Feedback provision\n",
      "- Error handling\n",
      "\n",
      "Define supporting functionalities required to enable these interactions, focusing on essential system functionalities such as:\n",
      "- Document analysis capabilities\n",
      "- Question answering algorithms\n",
      "- User feedback mechanisms\n",
      "- Error detection and response strategies\n",
      "Time taken: 4.353s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## META PROMPTING\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = (\"You are an expert in software requirement analysis. Your task is to generate a well-structured zero-shot prompt that guides an AI model to analyze and extract key requirements for a following solution: \"\n",
    "           \"A study companion discord bot that is to accept any document, analyse it, and answer questions based on the document. \"\n",
    "           \"The generated prompt must instruct the AI to Identify User Interactions (List all distinct interactions that users will perform within the system) \"\n",
    "           \"and Define Supporting Functionalities (Specify the essential system functionalities required to enable these interactions). \"\n",
    "           \"The generated prompt must exclude non-essential details such as system architecture, implementation details, or non-functional requirements, \"\n",
    "           \"and be formatted as a standalone prompt with no additional explanation, introductory text, or metadata. Output only the generated prompt and nothing else.\")\n",
    "\n",
    "#### The Prompt above has gone through multiple iterations of fine tuning along with help of automated prompting technique from ChatGPT. \n",
    "#### We set the GenAI's identity to help it narrow down its response pool, specify a desired prompting technique and request a prompt realted to Requirement Analysis.\n",
    "#### We then specify exact needs of our Requirement Analysis (User Interactions and Supporting Functionalities), and format it's output by cutting out any non essential analysis or text.\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "PROMPT = MESSAGE\n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, # Responsible for how \"random\" can the answer be. Keeping at 0.3 to keep prompt consistant yet allow for small variations.\n",
    "                         num_ctx=512, # Essentially responsible for models memory / context window. 512 allows the model to understand the request and create a consize response with some extra room if needed. Any higher and model goes off topic\n",
    "                         num_predict=250) # Responsible for how many word tokens are permitted to be returned. 250 keeps the prompt consize. Any higher makes a bloated prompt that yeilds inconsistent results.\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
